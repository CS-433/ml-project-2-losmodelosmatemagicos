{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT's playground \n",
    "Hello there! Welcome on BERT's playground. You may play with BERT here and see what he can do but always make sure he feels respected and admired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import masking\n",
    "import BERT\n",
    "\n",
    "from Vectorisation import Vectorisation\n",
    "from Config import Config\n",
    "from MaskedLanguageModel import MaskedLanguageModel\n",
    "from MaskedTextGenerator import MaskedTextGenerator\n",
    "\n",
    "with open(\"./ml4science_data.pkl\", \"rb\") as fp:\n",
    "    data_dict = pickle.load(fp)\n",
    "\n",
    "config = Config()\n",
    "vec = Vectorisation(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254, 128) (254, 128) (254, 128)\n",
      "<_BatchDataset element_spec=(TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), TensorSpec(shape=(None, 128), dtype=tf.float64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for masked language model\n",
    "encoded = vec.encode_dict(data_dict)\n",
    "x_masked_encoded, y_masked_encoded, sample_weights = masking.mask_input_and_labels(encoded, config.TOKEN_DICT)\n",
    "print(x_masked_encoded.shape, y_masked_encoded.shape, sample_weights.shape)\n",
    "\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices((x_masked_encoded, y_masked_encoded, sample_weights))\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)\n",
    "\n",
    "print(mlm_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26  2 26 21 21 26 23 25 21 23  2 21 26 21 21 26 21 25  3  8  3  3  3  3\n",
      "  3  3  3  3  7  8  4  2  8  3 16 20 21 26 21 23 21  3  8  7  8  3  5  8\n",
      "  4  8  2  8  3  8 10  9 10  9  9 15 20 11  9 14 13 11  9 14 13 14 10 11\n",
      "  9  9 13 14 13 14 11  9 10 14 13 10 11  9 14  9 14 15 20 15 15 17 20  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# TODO: continue implementing everything downstream so that BERT can finally run freely on his playground just as he wishes\n",
    "sample_tokens = x_masked_encoded\n",
    "print(encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26  2 26 21 21 26  9 25 21 23  2 21 26 21 21 26 21  1  3  8  3  1  3  3\n",
      "  3  3  3  3  7  8  1  2  8  3 16 20 21 26 21 23 21  3  8  7  8  3  5  8\n",
      "  4  1  2  8  3  1 10  9 10  9  9 15  1 11  9  1  1 11  9 14 13 14 10  1\n",
      "  9  9 13 14 13 14 11  9 10 14 13 10 11  1 14  9  1 15 20 15 15 17 20  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(x_masked_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"masked_bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " word_embedding (Embedding)  (None, 128, 256)             6912      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 128, 256)             0         ['word_embedding[0][0]']      \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " encoder_0/multiheadattenti  (None, 128, 256)             263168    ['tf.__operators__.add[0][0]',\n",
      " on (MultiHeadAttention)                                             'tf.__operators__.add[0][0]',\n",
      "                                                                     'tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " encoder_0/att_dropout (Dro  (None, 128, 256)             0         ['encoder_0/multiheadattention\n",
      " pout)                                                              [0][0]']                      \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 128, 256)             0         ['tf.__operators__.add[0][0]',\n",
      " OpLambda)                                                           'encoder_0/att_dropout[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_0/att_layernormali  (None, 128, 256)             512       ['tf.__operators__.add_1[0][0]\n",
      " zation (LayerNormalization                                         ']                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " encoder_0/ffn (Sequential)  (None, 128, 256)             65920     ['encoder_0/att_layernormaliza\n",
      "                                                                    tion[0][0]']                  \n",
      "                                                                                                  \n",
      " encoder_0/ffn_dropout (Dro  (None, 128, 256)             0         ['encoder_0/ffn[0][0]']       \n",
      " pout)                                                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 128, 256)             0         ['encoder_0/att_layernormaliza\n",
      " OpLambda)                                                          tion[0][0]',                  \n",
      "                                                                     'encoder_0/ffn_dropout[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_0/ffn_layernormali  (None, 128, 256)             512       ['tf.__operators__.add_2[0][0]\n",
      " zation (LayerNormalization                                         ']                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " mlm_cls (Dense)             (None, 128, 27)              6939      ['encoder_0/ffn_layernormaliza\n",
      "                                                                    tion[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 343965 (1.31 MB)\n",
      "Trainable params: 343963 (1.31 MB)\n",
      "Non-trainable params: 2 (8.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_callback = MaskedTextGenerator(sample_tokens, config.TOKEN_DICT['[MASK]'])\n",
    "\n",
    "bert_masked_model = BERT.create_masked_language_bert_model(config)\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 2s 148ms/step\n",
      "(254, 128, 27)\n",
      "(11, 27)\n",
      "[21 26 10 22 14]\n",
      "{'input_seq': array([26,  2, 26, 21, 21, 26,  9, 25, 21, 23,  2, 21, 26, 21, 21, 26, 21,\n",
      "        1,  3,  8,  3,  1,  3,  3,  3,  3,  3,  3,  7,  8,  1,  2,  8,  3,\n",
      "       16, 20, 21, 26, 21, 23, 21,  3,  8,  7,  8,  3,  5,  8,  4,  1,  2,\n",
      "        8,  3,  1, 10,  9, 10,  9,  9, 15,  1, 11,  9,  1,  1, 11,  9, 14,\n",
      "       13, 14, 10,  1,  9,  9, 13, 14, 13, 14, 11,  9, 10, 14, 13, 10, 11,\n",
      "        1, 14,  9,  1, 15, 20, 15, 15, 17, 20,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
      " 'prediction': array([21, 26, 10, 22, 14], dtype=int64),\n",
      " 'probability': array([0.4080506 , 0.21907467, 0.0528652 , 0.05080076, 0.04285622],\n",
      "      dtype=float32)}\n",
      "16/16 [==============================] - 11s 435ms/step - loss: 3.0728\n",
      "Epoch 2/5\n",
      "8/8 [==============================] - 2s 215ms/step\n",
      "(254, 128, 27)\n",
      "(11, 27)\n",
      "[26 21 22 23 14]\n",
      "{'input_seq': array([26,  2, 26, 21, 21, 26,  9, 25, 21, 23,  2, 21, 26, 21, 21, 26, 21,\n",
      "        1,  3,  8,  3,  1,  3,  3,  3,  3,  3,  3,  7,  8,  1,  2,  8,  3,\n",
      "       16, 20, 21, 26, 21, 23, 21,  3,  8,  7,  8,  3,  5,  8,  4,  1,  2,\n",
      "        8,  3,  1, 10,  9, 10,  9,  9, 15,  1, 11,  9,  1,  1, 11,  9, 14,\n",
      "       13, 14, 10,  1,  9,  9, 13, 14, 13, 14, 11,  9, 10, 14, 13, 10, 11,\n",
      "        1, 14,  9,  1, 15, 20, 15, 15, 17, 20,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
      " 'prediction': array([26, 21, 22, 23, 14], dtype=int64),\n",
      " 'probability': array([0.38319537, 0.22529046, 0.07902929, 0.07408266, 0.049788  ],\n",
      "      dtype=float32)}\n",
      "16/16 [==============================] - 7s 449ms/step - loss: 2.6105\n",
      "Epoch 3/5\n",
      "8/8 [==============================] - 2s 197ms/step\n",
      "(254, 128, 27)\n",
      "(11, 27)\n",
      "[21 26 22  9 23]\n",
      "{'input_seq': array([26,  2, 26, 21, 21, 26,  9, 25, 21, 23,  2, 21, 26, 21, 21, 26, 21,\n",
      "        1,  3,  8,  3,  1,  3,  3,  3,  3,  3,  3,  7,  8,  1,  2,  8,  3,\n",
      "       16, 20, 21, 26, 21, 23, 21,  3,  8,  7,  8,  3,  5,  8,  4,  1,  2,\n",
      "        8,  3,  1, 10,  9, 10,  9,  9, 15,  1, 11,  9,  1,  1, 11,  9, 14,\n",
      "       13, 14, 10,  1,  9,  9, 13, 14, 13, 14, 11,  9, 10, 14, 13, 10, 11,\n",
      "        1, 14,  9,  1, 15, 20, 15, 15, 17, 20,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
      " 'prediction': array([21, 26, 22,  9, 23], dtype=int64),\n",
      " 'probability': array([0.3457039 , 0.14557946, 0.10211164, 0.08642954, 0.07118937],\n",
      "      dtype=float32)}\n",
      "16/16 [==============================] - 7s 429ms/step - loss: 2.5369\n",
      "Epoch 4/5\n",
      "8/8 [==============================] - 1s 134ms/step\n",
      "(254, 128, 27)\n",
      "(11, 27)\n",
      "[21 26 14  9 22]\n",
      "{'input_seq': array([26,  2, 26, 21, 21, 26,  9, 25, 21, 23,  2, 21, 26, 21, 21, 26, 21,\n",
      "        1,  3,  8,  3,  1,  3,  3,  3,  3,  3,  3,  7,  8,  1,  2,  8,  3,\n",
      "       16, 20, 21, 26, 21, 23, 21,  3,  8,  7,  8,  3,  5,  8,  4,  1,  2,\n",
      "        8,  3,  1, 10,  9, 10,  9,  9, 15,  1, 11,  9,  1,  1, 11,  9, 14,\n",
      "       13, 14, 10,  1,  9,  9, 13, 14, 13, 14, 11,  9, 10, 14, 13, 10, 11,\n",
      "        1, 14,  9,  1, 15, 20, 15, 15, 17, 20,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
      " 'prediction': array([21, 26, 14,  9, 22], dtype=int64),\n",
      " 'probability': array([0.30149236, 0.23519681, 0.10595968, 0.07999912, 0.06744   ],\n",
      "      dtype=float32)}\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 2.4939\n",
      "Epoch 5/5\n",
      "8/8 [==============================] - 1s 169ms/step\n",
      "(254, 128, 27)\n",
      "(11, 27)\n",
      "[21 26 23 22 14]\n",
      "{'input_seq': array([26,  2, 26, 21, 21, 26,  9, 25, 21, 23,  2, 21, 26, 21, 21, 26, 21,\n",
      "        1,  3,  8,  3,  1,  3,  3,  3,  3,  3,  3,  7,  8,  1,  2,  8,  3,\n",
      "       16, 20, 21, 26, 21, 23, 21,  3,  8,  7,  8,  3,  5,  8,  4,  1,  2,\n",
      "        8,  3,  1, 10,  9, 10,  9,  9, 15,  1, 11,  9,  1,  1, 11,  9, 14,\n",
      "       13, 14, 10,  1,  9,  9, 13, 14, 13, 14, 11,  9, 10, 14, 13, 10, 11,\n",
      "        1, 14,  9,  1, 15, 20, 15, 15, 17, 20,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
      " 'prediction': array([21, 26, 23, 22, 14], dtype=int64),\n",
      " 'probability': array([0.2905095 , 0.21270119, 0.14405978, 0.0831576 , 0.07186581],\n",
      "      dtype=float32)}\n",
      "16/16 [==============================] - 6s 386ms/step - loss: 2.4366\n"
     ]
    }
   ],
   "source": [
    "bert_masked_model.fit(mlm_ds, epochs=5, callbacks=[generator_callback])\n",
    "bert_masked_model.save(\"bert_mlm.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

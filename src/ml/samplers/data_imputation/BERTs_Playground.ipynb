{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT's playground \n",
    "Hello there! Welcome on BERT's playground. You may play with BERT here and see what he can do but always make sure he feels respected and admired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import masking\n",
    "import BERT\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from Vectorisation import Vectorisation\n",
    "from Config import Config\n",
    "from MaskedTextGenerator import MaskedTextGenerator\n",
    "\n",
    "with open(\"./ml4science_data.pkl\", \"rb\") as fp:\n",
    "    data_dict = pickle.load(fp)\n",
    "\n",
    "config = Config()\n",
    "vec = Vectorisation(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254, 128) (254, 128) (254, 128)\n",
      "<_BatchDataset element_spec=(TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), TensorSpec(shape=(None, 128), dtype=tf.float64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for masked language model\n",
    "encoded = vec.encode_dict(data_dict)\n",
    "x_masked_encoded, y_masked_encoded, sample_weights = masking.mask_input_and_labels(encoded, config.TOKEN_DICT)\n",
    "print(x_masked_encoded.shape, y_masked_encoded.shape, sample_weights.shape)\n",
    "\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices((x_masked_encoded, y_masked_encoded, sample_weights))\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)\n",
    "\n",
    "print(mlm_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26  2 26 21 21 26 23 25 21 23  2 21 26 21 21 26 21 25  3  8  3  3  3  3\n",
      "  3  3  3  3  7  8  4  2  8  3 16 20 21 26 21 23 21  3  8  7  8  3  5  8\n",
      "  4  8  2  8  3  8 10  9 10  9  9 15 20 11  9 14 13 11  9 14 13 14 10 11\n",
      "  9  9 13 14 13 14 11  9 10 14 13 10 11  9 14  9 14 15 20 15 15 17 20  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "sample_tokens = x_masked_encoded[0:1]\n",
    "print(y_masked_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26  2 26 21 21 26  9 25 21 23  2 21 26 21 21 26 21  1  3  8  3  1  3  3\n",
      "  3  3  3  3  7  8  1  2  8  3 16 20 21 26 21 23 21  3  8  7  8  3  5  8\n",
      "  4  1  2  8  3  1 10  9 10  9  9 15  1 11  9  1  1 11  9 14 13 14 10  1\n",
      "  9  9 13 14 13 14 11  9 10 14 13 10 11  1 14  9  1 15 20 15 15 17 20  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(x_masked_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"masked_bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)       [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " word_embedding (Embedding)  (None, 128, 64)              1728      ['input_10[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TF  (None, 128, 64)              0         ['word_embedding[0][0]']      \n",
      " OpLambda)                                                                                        \n",
      "                                                                                                  \n",
      " encoder_0/multiheadattenti  (None, 128, 64)              16640     ['tf.__operators__.add_9[0][0]\n",
      " on (MultiHeadAttention)                                            ',                            \n",
      "                                                                     'tf.__operators__.add_9[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'tf.__operators__.add_9[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " encoder_0/att_dropout (Dro  (None, 128, 64)              0         ['encoder_0/multiheadattention\n",
      " pout)                                                              [0][0]']                      \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (T  (None, 128, 64)              0         ['tf.__operators__.add_9[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'encoder_0/att_dropout[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_0/att_layernormali  (None, 128, 64)              128       ['tf.__operators__.add_10[0][0\n",
      " zation (LayerNormalization                                         ]']                           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " encoder_0/ffn (Sequential)  (None, 128, 64)              8320      ['encoder_0/att_layernormaliza\n",
      "                                                                    tion[0][0]']                  \n",
      "                                                                                                  \n",
      " encoder_0/ffn_dropout (Dro  (None, 128, 64)              0         ['encoder_0/ffn[0][0]']       \n",
      " pout)                                                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (T  (None, 128, 64)              0         ['encoder_0/att_layernormaliza\n",
      " FOpLambda)                                                         tion[0][0]',                  \n",
      "                                                                     'encoder_0/ffn_dropout[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_0/ffn_layernormali  (None, 128, 64)              128       ['tf.__operators__.add_11[0][0\n",
      " zation (LayerNormalization                                         ]']                           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " mlm_cls (Dense)             (None, 128, 27)              1755      ['encoder_0/ffn_layernormaliza\n",
      "                                                                    tion[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 28701 (112.11 KB)\n",
      "Trainable params: 28699 (112.11 KB)\n",
      "Non-trainable params: 2 (8.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_callback = MaskedTextGenerator(sample_tokens, config.TOKEN_DICT['[MASK]'])\n",
    "\n",
    "bert_masked_model = BERT.create_masked_language_bert_model(config)\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 3/16 [====>.........................] - ETA: 0s - loss: 0.9096"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/steploss: 0.91\n",
      "prediction shape: (1, 128, 27)\n",
      "mask pred shape: (11, 27)\n",
      "best results shape: (5, 3)\n",
      "\n",
      "masked nb: \t 1                2                3                4                5                \n",
      "predictions: \t[25 21 23]       [ 3  8 23]       [4 8 5]       [8 3 4]       [8 3 4]       \n",
      "probabilities: \t[0.4  0.2  0.17] [0.63 0.13 0.05] [0.54 0.27 0.12] [0.83 0.07 0.04] [0.71 0.25 0.02] \n",
      "\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.9139\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 34ms/steploss: 0.90\n",
      "prediction shape: (1, 128, 27)\n",
      "mask pred shape: (11, 27)\n",
      "best results shape: (5, 3)\n",
      "\n",
      "masked nb: \t 1                2                3                4                5                \n",
      "predictions: \t[25 26 23]       [ 3 26 23]       [4 8 5]       [8 3 4]       [8 3 4]       \n",
      "probabilities: \t[0.48 0.25 0.11] [0.42 0.17 0.12] [0.61 0.2  0.12] [0.86 0.05 0.03] [0.76 0.18 0.02] \n",
      "\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.9000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 40ms/steploss: 0.88\n",
      "prediction shape: (1, 128, 27)\n",
      "mask pred shape: (11, 27)\n",
      "best results shape: (5, 3)\n",
      "\n",
      "masked nb: \t 1                2                3                4                5                \n",
      "predictions: \t[25 26 21]       [ 3  8 26]       [4 8 5]       [8 4 5]       [8 3 5]       \n",
      "probabilities: \t[0.34 0.23 0.17] [0.54 0.18 0.05] [0.58 0.26 0.1 ] [0.84 0.05 0.04] [0.83 0.11 0.02] \n",
      "\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.8868\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 38ms/steploss: 0.86\n",
      "prediction shape: (1, 128, 27)\n",
      "mask pred shape: (11, 27)\n",
      "best results shape: (5, 3)\n",
      "\n",
      "masked nb: \t 1                2                3                4                5                \n",
      "predictions: \t[25 21 26]       [ 3  8 25]       [4 8 5]       [8 3 4]       [8 3 5]       \n",
      "probabilities: \t[0.54 0.18 0.13] [0.63 0.14 0.08] [0.53 0.3  0.12] [0.87 0.05 0.03] [0.77 0.18 0.02] \n",
      "\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.8682\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 43ms/steploss: 0.87\n",
      "prediction shape: (1, 128, 27)\n",
      "mask pred shape: (11, 27)\n",
      "best results shape: (5, 3)\n",
      "\n",
      "masked nb: \t 1                2                3                4                5                \n",
      "predictions: \t[25 26 23]       [ 3 25  8]       [4 8 5]       [8 4 5]       [8 3 4]       \n",
      "probabilities: \t[0.69 0.11 0.09] [0.7  0.09 0.08] [0.74 0.15 0.07] [0.79 0.1  0.04] [0.77 0.15 0.04] \n",
      "\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 0.8691\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 32ms/steploss: 0.86\n",
      "prediction shape: (1, 128, 27)\n",
      "mask pred shape: (11, 27)\n",
      "best results shape: (5, 3)\n",
      "\n",
      "masked nb: \t 1                2                3                4                5                \n",
      "predictions: \t[25 21 26]       [ 3  8 25]       [4 8 5]       [8 4 5]       [8 3 4]       \n",
      "probabilities: \t[0.53 0.18 0.14] [0.79 0.1  0.04] [0.73 0.16 0.07] [0.86 0.06 0.04] [0.85 0.1  0.02] \n",
      "\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.8726\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 30ms/steploss: 0.83\n",
      "prediction shape: (1, 128, 27)\n",
      "mask pred shape: (11, 27)\n",
      "best results shape: (5, 3)\n",
      "\n",
      "masked nb: \t 1                2                3                4                5                \n",
      "predictions: \t[25 26 21]       [ 3  8 25]       [4 8 5]       [8 4 5]       [8 3 4]       \n",
      "probabilities: \t[0.37 0.22 0.21] [0.77 0.06 0.04] [0.67 0.2  0.09] [0.86 0.05 0.03] [0.82 0.12 0.02] \n",
      "\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.8418\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 26ms/steploss: 0.84\n",
      "prediction shape: (1, 128, 27)\n",
      "mask pred shape: (11, 27)\n",
      "best results shape: (5, 3)\n",
      "\n",
      "masked nb: \t 1                2                3                4                5                \n",
      "predictions: \t[25 26 21]       [ 3 26 25]       [4 8 5]       [8 4 3]       [8 3 4]       \n",
      "probabilities: \t[0.5  0.21 0.14] [0.73 0.08 0.07] [0.7  0.2  0.07] [0.89 0.04 0.03] [0.81 0.15 0.02] \n",
      "\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.8423\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 35ms/steploss: 0.83\n",
      "prediction shape: (1, 128, 27)\n",
      "mask pred shape: (11, 27)\n",
      "best results shape: (5, 3)\n",
      "\n",
      "masked nb: \t 1                2                3                4                5                \n",
      "predictions: \t[25 26 21]       [ 3 25 26]       [4 8 5]       [8 4 5]       [8 3 4]       \n",
      "probabilities: \t[0.41 0.26 0.15] [0.75 0.06 0.05] [0.87 0.07 0.04] [0.79 0.1  0.05] [0.7  0.22 0.03] \n",
      "\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.8318\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 44ms/steploss: 0.82\n",
      "prediction shape: (1, 128, 27)\n",
      "mask pred shape: (11, 27)\n",
      "best results shape: (5, 3)\n",
      "\n",
      "masked nb: \t 1                2                3                4                5                \n",
      "predictions: \t[25 21 26]       [ 3 26 23]       [4 8 5]       [8 4 5]       [8 3 4]       \n",
      "probabilities: \t[0.37 0.25 0.21] [0.53 0.13 0.09] [0.75 0.14 0.08] [0.88 0.04 0.02] [0.85 0.11 0.02] \n",
      "\n",
      "16/16 [==============================] - 1s 42ms/step - loss: 0.8226\n"
     ]
    }
   ],
   "source": [
    "# A callback in Keras is a function that is called at certain points during the training process. -> Here is called after each epoch during the training\n",
    "# Here we use to see the \"performance\" at each epoch while predicting on a \"test set\" aka the sample_tokens\n",
    "bert_masked_model.fit(mlm_ds, epochs=10, callbacks=[generator_callback]) \n",
    "bert_masked_model.save(\"bert_models/bert_mlm.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predictions:\n",
      " [[26 21 26 21 21 26 21 25 21 23 21 21 26 21 21 26 21 25  3 21  3  3  3  3\n",
      "   3  3  3  3  8  8  4  4  8  5 16 20  8 15 15  7  3  3  8  7  8  3  4  8\n",
      "   8  8  8  8  3  8 10  9 10  9  9 13 14 11  9 14 14 11  9 14 14 14 10 11\n",
      "  11 11 14 14 13 14 11  9 10 14 13  9 11  9 14  9 14 14 20  9 15 17 20 11\n",
      "  11 11  9  9  9 20 20 20 20 20 20 20 20 20 26  4  4 15 15 15 15 20 15 20\n",
      "  20 26 26 26  3  3  9 14]]\n",
      "Original:\n",
      " [[26  2 26 21 21 26 23 25 21 23  2 21 26 21 21 26 21 25  3  8  3  3  3  3\n",
      "   3  3  3  3  7  8  4  2  8  3 16 20 21 26 21 23 21  3  8  7  8  3  5  8\n",
      "   4  8  2  8  3  8 10  9 10  9  9 15 20 11  9 14 13 11  9 14 13 14 10 11\n",
      "   9  9 13 14 13 14 11  9 10 14 13 10 11  9 14  9 14 15 20 15 15 17 20  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]]\n",
      "Masked:\n",
      " [[26  2 26 21 21 26  9 25 21 23  2 21 26 21 21 26 21  1  3  8  3  1  3  3\n",
      "   3  3  3  3  7  8  1  2  8  3 16 20 21 26 21 23 21  3  8  7  8  3  5  8\n",
      "   4  1  2  8  3  1 10  9 10  9  9 15  1 11  9  1  1 11  9 14 13 14 10  1\n",
      "   9  9 13 14 13 14 11  9 10 14 13 10 11  1 14  9  1 15 20 15 15 17 20  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "predictions = bert_masked_model.predict(x_masked_encoded[0:1])\n",
    "\n",
    "predictions_max = np.argmax(predictions, axis=2)\n",
    "print(\"Predictions:\\n\",predictions_max)\n",
    "print(\"Original:\\n\", y_masked_encoded[0:1])\n",
    "print(\"Masked:\\n\", x_masked_encoded[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True False  True  True  True  True False  True  True  True False  True\n",
      "   True  True  True  True  True  True  True False  True  True  True  True\n",
      "   True  True  True  True False  True  True False  True False  True  True\n",
      "  False False False False False  True  True  True  True  True False  True\n",
      "  False  True False  True  True  True  True  True  True  True  True False\n",
      "  False  True  True  True False  True  True  True False  True  True  True\n",
      "  False False False  True  True  True  True  True  True  True  True False\n",
      "   True  True  True  True  True False  True False  True  True  True False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False]]\n",
      "Accuracy brut:  70.0\n",
      "Accuracy without taking in acount padding:  0.7368421052631579\n"
     ]
    }
   ],
   "source": [
    "print(predictions_max == y_masked_encoded[0:1])\n",
    "# print accuracy\n",
    "print(\"Accuracy brut: \", np.sum(predictions_max == y_masked_encoded[0:1]) / len(y_masked_encoded[0:1]))\n",
    "# print accuracy without padding\n",
    "print(\"Accuracy without taking in acount padding: \", np.sum((predictions_max == y_masked_encoded[0:1]) * (y_masked_encoded[0:1] != 0)) / np.sum(y_masked_encoded[0:1] != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This how we can load a Keras model\n",
    "\"\"\"# Load OUR pretrained bert model\n",
    "mlm_model = keras.models.load_model(\n",
    "    \"bert_mlm_imdb.h5\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel}\n",
    ")\"\"\"\n",
    "\n",
    "# Kinda failed attempt to create a end to end model (we don't really need it) -> But it's a good example of how to create a model with a custom loss function and reshape the output\n",
    "\"\"\"def get_end_to_end(model):\n",
    "    inputs = keras.Input(shape=(None,))\n",
    "    outputs = model(inputs)\n",
    "    reshaped_outputs = keras.layers.Lambda(lambda x: keras.backend.argmax(x, axis=-1))(outputs)\n",
    "    end_to_end_model = keras.Model(inputs, reshaped_outputs, name=\"end_to_end_model\")\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.bert.LR)\n",
    "    end_to_end_model.compile(\n",
    "        optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return end_to_end_model\n",
    "\n",
    "end_to_end_classification_model = get_end_to_end(bert_masked_model)\n",
    "\n",
    "# Build dataset for end to end model input (will be used at the end)\n",
    "test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices((x_masked_encoded, y_masked_encoded)).batch(config.BATCH_SIZE)\n",
    "\n",
    "end_to_end_classification_model.evaluate(test_raw_classifier_ds)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

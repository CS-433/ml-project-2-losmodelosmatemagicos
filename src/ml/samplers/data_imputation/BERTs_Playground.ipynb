{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT's playground \n",
    "Hello there! Welcome on BERT's playground. You may play with BERT here and see what he can do but always make sure he feels respected and admired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import masking\n",
    "import BERT\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from Vectorisation import Vectorisation\n",
    "from Config import Config\n",
    "from MaskedTextGenerator import MaskedTextGenerator\n",
    "\n",
    "with open(\"./ml4science_data.pkl\", \"rb\") as fp:\n",
    "    data_dict = pickle.load(fp)\n",
    "\n",
    "config = Config()\n",
    "vec = Vectorisation(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254, 128) (254, 128) (254, 128)\n",
      "<_BatchDataset element_spec=(TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), TensorSpec(shape=(None, 128), dtype=tf.float64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for masked language model\n",
    "encoded = vec.encode_dict(data_dict)\n",
    "x_masked_encoded, y_masked_encoded, sample_weights = masking.mask_input_and_labels(encoded, config.TOKEN_DICT)\n",
    "print(x_masked_encoded.shape, y_masked_encoded.shape, sample_weights.shape)\n",
    "\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices((x_masked_encoded, y_masked_encoded, sample_weights))\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)\n",
    "\n",
    "print(mlm_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26  2 26 21 21 26 23 25 21 23  2 21 26 21 21 26 21 25  3  8  3  3  3  3\n",
      "  3  3  3  3  7  8  4  2  8  3 16 20 21 26 21 23 21  3  8  7  8  3  5  8\n",
      "  4  8  2  8  3  8 10  9 10  9  9 15 20 11  9 14 13 11  9 14 13 14 10 11\n",
      "  9  9 13 14 13 14 11  9 10 14 13 10 11  9 14  9 14 15 20 15 15 17 20  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "sample_tokens = x_masked_encoded[0:1]\n",
    "print(y_masked_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26  2 26 21 21 26  9 25 21 23  2 21 26 21 21 26 21  1  3  8  3  1  3  3\n",
      "  3  3  3  3  7  8  1  2  8  3 16 20 21 26 21 23 21  3  8  7  8  3  5  8\n",
      "  4  1  2  8  3  1 10  9 10  9  9 15  1 11  9  1  1 11  9 14 13 14 10  1\n",
      "  9  9 13 14 13 14 11  9 10 14 13 10 11  1 14  9  1 15 20 15 15 17 20  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(x_masked_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"masked_bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)       [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " word_embedding (Embedding)  (None, 128, 64)              1728      ['input_10[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TF  (None, 128, 64)              0         ['word_embedding[0][0]']      \n",
      " OpLambda)                                                                                        \n",
      "                                                                                                  \n",
      " encoder_0/multiheadattenti  (None, 128, 64)              16640     ['tf.__operators__.add_9[0][0]\n",
      " on (MultiHeadAttention)                                            ',                            \n",
      "                                                                     'tf.__operators__.add_9[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'tf.__operators__.add_9[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " encoder_0/att_dropout (Dro  (None, 128, 64)              0         ['encoder_0/multiheadattention\n",
      " pout)                                                              [0][0]']                      \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (T  (None, 128, 64)              0         ['tf.__operators__.add_9[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'encoder_0/att_dropout[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_0/att_layernormali  (None, 128, 64)              128       ['tf.__operators__.add_10[0][0\n",
      " zation (LayerNormalization                                         ]']                           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " encoder_0/ffn (Sequential)  (None, 128, 64)              8320      ['encoder_0/att_layernormaliza\n",
      "                                                                    tion[0][0]']                  \n",
      "                                                                                                  \n",
      " encoder_0/ffn_dropout (Dro  (None, 128, 64)              0         ['encoder_0/ffn[0][0]']       \n",
      " pout)                                                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (T  (None, 128, 64)              0         ['encoder_0/att_layernormaliza\n",
      " FOpLambda)                                                         tion[0][0]',                  \n",
      "                                                                     'encoder_0/ffn_dropout[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_0/ffn_layernormali  (None, 128, 64)              128       ['tf.__operators__.add_11[0][0\n",
      " zation (LayerNormalization                                         ]']                           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " mlm_cls (Dense)             (None, 128, 27)              1755      ['encoder_0/ffn_layernormaliza\n",
      "                                                                    tion[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 28701 (112.11 KB)\n",
      "Trainable params: 28699 (112.11 KB)\n",
      "Non-trainable params: 2 (8.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_callback = MaskedTextGenerator(sample_tokens, config.TOKEN_DICT['[MASK]'])\n",
    "\n",
    "bert_masked_model = BERT.create_masked_language_bert_model(config)\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 205ms/steposs: 3.15\n",
      "prediction shape: (1, 128, 27)\n",
      "mask pred shape: (11, 27)\n",
      "best results shape: (5, 3)\n",
      "\n",
      "masked nb: \t 1                2                3                4                5                \n",
      "predictions: \t[26 21 10]       [26 21  9]       [26 21 10]       [21 22 10]       [26 22  9]       \n",
      "probabilities: \t[0.31 0.14 0.12] [0.36 0.1  0.09] [0.23 0.15 0.13] [0.28 0.16 0.15] [0.22 0.16 0.11] \n",
      "\n",
      "16/16 [==============================] - 3s 43ms/step - loss: 3.1686\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 27ms/steploss: 2.73\n",
      "prediction shape: (1, 128, 27)\n",
      "mask pred shape: (11, 27)\n",
      "best results shape: (5, 3)\n",
      "\n",
      "masked nb: \t 1                2                3                4                5                \n",
      "predictions: \t[26 21 10]       [26 21 14]       [26 21 10]       [21 22 20]       [26 20 21]       \n",
      "probabilities: \t[0.3  0.23 0.07] [0.31 0.15 0.09] [0.17 0.15 0.08] [0.22 0.12 0.1 ] [0.16 0.12 0.12] \n",
      "\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 2.7273\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 30ms/steploss: 2.59\n",
      "prediction shape: (1, 128, 27)\n",
      "mask pred shape: (11, 27)\n",
      "best results shape: (5, 3)\n",
      "\n",
      "masked nb: \t 1                2                3                4                5                \n",
      "predictions: \t[26 21  9]       [26 21  9]       [26 21 22]       [21 22 10]       [26 22 21]       \n",
      "probabilities: \t[0.31 0.26 0.07] [0.3  0.19 0.08] [0.16 0.14 0.1 ] [0.19 0.14 0.09] [0.14 0.13 0.11] \n",
      "\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 2.5980\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 23ms/steploss: 2.55\n",
      "prediction shape: (1, 128, 27)\n",
      "mask pred shape: (11, 27)\n",
      "best results shape: (5, 3)\n",
      "\n",
      "masked nb: \t 1                2                3                4                5                \n",
      "predictions: \t[21 26 22]       [21 26 14]       [21 22 26]       [22 21 10]       [22 21 26]       \n",
      "probabilities: \t[0.31 0.22 0.07] [0.22 0.22 0.1 ] [0.13 0.12 0.11] [0.17 0.16 0.1 ] [0.15 0.1  0.09] \n",
      "\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 2.5502\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 30ms/steploss: 2.51\n",
      "prediction shape: (1, 128, 27)\n",
      "mask pred shape: (11, 27)\n",
      "best results shape: (5, 3)\n",
      "\n",
      "masked nb: \t 1                2                3                4                5                \n",
      "predictions: \t[21 26 23]       [26 21  9]       [22 10 26]       [22 21 10]       [22  9 20]       \n",
      "probabilities: \t[0.24 0.22 0.09] [0.21 0.16 0.11] [0.14 0.1  0.1 ] [0.19 0.1  0.1 ] [0.15 0.12 0.11] \n",
      "\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 2.5228\n"
     ]
    }
   ],
   "source": [
    "# A callback in Keras is a function that is called at certain points during the training process. -> Here is called after each epoch during the training\n",
    "# Here we use to see the \"performance\" at each epoch while predicting on a \"test set\" aka the sample_tokens\n",
    "bert_masked_model.fit(mlm_ds, epochs=5, callbacks=[generator_callback]) \n",
    "bert_masked_model.save(\"bert_models/bert_mlm.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "Predictions:\n",
      " [[26 26 26 21 21 21 21 21 21 21 21 21 26 21 21 26 21 21 26 26 26 26 26 21\n",
      "  26 26 14 14 14 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 14 22 22\n",
      "  22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 14 14 14 20 20\n",
      "  22 22 22 22 22 22 22 22 10 22 22 22 22 22 22 22 22 22 22 22 22 22 20 20\n",
      "  10 10 10 10 22 22 22 22 22 22 22 20 20 20  9 10 20 20 20 22 22 22 22 22\n",
      "  22 22 22 22 10 10 10 10]]\n",
      "Original:\n",
      " [[26  2 26 21 21 26  9 25 21 23  2 21 26 21 21 26 21  1  3  8  3  1  3  3\n",
      "   3  3  3  3  7  8  1  2  8  3 16 20 21 26 21 23 21  3  8  7  8  3  5  8\n",
      "   4  1  2  8  3  1 10  9 10  9  9 15  1 11  9  1  1 11  9 14 13 14 10  1\n",
      "   9  9 13 14 13 14 11  9 10 14 13 10 11  1 14  9  1 15 20 15 15 17 20  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "predictions = bert_masked_model.predict(x_masked_encoded[0:1])\n",
    "\n",
    "predictions_max = np.argmax(predictions, axis=2)\n",
    "print(\"Predictions:\\n\",predictions_max)\n",
    "\n",
    "print(\"Original:\\n\", x_masked_encoded[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False,  True,  True,  True, False, False, False,  True,\n",
       "        False, False,  True,  True,  True,  True,  True,  True, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True, False,  True, False, False,\n",
       "        False, False, False, False, False, False, False, False,  True,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_max == y_masked_encoded[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This how we can load a Keras model\n",
    "\"\"\"# Load OUR pretrained bert model\n",
    "mlm_model = keras.models.load_model(\n",
    "    \"bert_mlm_imdb.h5\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel}\n",
    ")\"\"\"\n",
    "\n",
    "# Kinda failed attempt to create a end to end model (we don't really need it) -> But it's a good example of how to create a model with a custom loss function and reshape the output\n",
    "\"\"\"def get_end_to_end(model):\n",
    "    inputs = keras.Input(shape=(None,))\n",
    "    outputs = model(inputs)\n",
    "    reshaped_outputs = keras.layers.Lambda(lambda x: keras.backend.argmax(x, axis=-1))(outputs)\n",
    "    end_to_end_model = keras.Model(inputs, reshaped_outputs, name=\"end_to_end_model\")\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.bert.LR)\n",
    "    end_to_end_model.compile(\n",
    "        optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return end_to_end_model\n",
    "\n",
    "end_to_end_classification_model = get_end_to_end(bert_masked_model)\n",
    "\n",
    "# Build dataset for end to end model input (will be used at the end)\n",
    "test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices((x_masked_encoded, y_masked_encoded)).batch(config.BATCH_SIZE)\n",
    "\n",
    "end_to_end_classification_model.evaluate(test_raw_classifier_ds)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

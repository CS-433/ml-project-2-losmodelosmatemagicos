{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT's playground \n",
    "Hello there! Welcome on BERT's playground. You may play with BERT here and see what he can do but always make sure he feels respected and admired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import masking\n",
    "import BERT\n",
    "\n",
    "from Vectorisation import Vectorisation\n",
    "from Config import Config\n",
    "from MaskedLanguageModel import MaskedLanguageModel\n",
    "from MaskedTextGenerator import MaskedTextGenerator\n",
    "\n",
    "with open(\"./ml4science_data.pkl\", \"rb\") as fp:\n",
    "    data_dict = pickle.load(fp)\n",
    "\n",
    "config = Config()\n",
    "vec = Vectorisation(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254, 128) (254, 128) (254, 128)\n",
      "<_BatchDataset element_spec=(TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), TensorSpec(shape=(None, 128), dtype=tf.float64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for masked language model\n",
    "encoded = vec.encode_dict(data_dict)\n",
    "x_masked_encoded, y_masked_encoded, sample_weights = masking.mask_input_and_labels(encoded, config.TOKEN_DICT)\n",
    "print(x_masked_encoded.shape, y_masked_encoded.shape, sample_weights.shape)\n",
    "\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices((x_masked_encoded, y_masked_encoded, sample_weights))\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)\n",
    "\n",
    "print(mlm_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26  2 26 ...  0  0  0]\n",
      " [26 21 26 ... 14 11 14]\n",
      " [25 26 21 ...  0  0  0]\n",
      " ...\n",
      " [26  2 26 ...  0  0  0]\n",
      " [26 25 26 ...  0  0  0]\n",
      " [26  2  1 ... 22 26 21]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: continue implementing everything downstream so that BERT can finally run freely on his playground just as he wishes\n",
    "sample_tokens = x_masked_encoded\n",
    "print(sample_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"masked_bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " word_embedding (Embedding)  (None, 128, 256)             6912      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 128, 256)             0         ['word_embedding[0][0]']      \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " encoder_0/multiheadattenti  (None, 128, 256)             263168    ['tf.__operators__.add[0][0]',\n",
      " on (MultiHeadAttention)                                             'tf.__operators__.add[0][0]',\n",
      "                                                                     'tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " encoder_0/att_dropout (Dro  (None, 128, 256)             0         ['encoder_0/multiheadattention\n",
      " pout)                                                              [0][0]']                      \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 128, 256)             0         ['tf.__operators__.add[0][0]',\n",
      " OpLambda)                                                           'encoder_0/att_dropout[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_0/att_layernormali  (None, 128, 256)             512       ['tf.__operators__.add_1[0][0]\n",
      " zation (LayerNormalization                                         ']                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " encoder_0/ffn (Sequential)  (None, 128, 256)             65920     ['encoder_0/att_layernormaliza\n",
      "                                                                    tion[0][0]']                  \n",
      "                                                                                                  \n",
      " encoder_0/ffn_dropout (Dro  (None, 128, 256)             0         ['encoder_0/ffn[0][0]']       \n",
      " pout)                                                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 128, 256)             0         ['encoder_0/att_layernormaliza\n",
      " OpLambda)                                                          tion[0][0]',                  \n",
      "                                                                     'encoder_0/ffn_dropout[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_0/ffn_layernormali  (None, 128, 256)             512       ['tf.__operators__.add_2[0][0]\n",
      " zation (LayerNormalization                                         ']                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " mlm_cls (Dense)             (None, 128, 27)              6939      ['encoder_0/ffn_layernormaliza\n",
      "                                                                    tion[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 343965 (1.31 MB)\n",
      "Trainable params: 343963 (1.31 MB)\n",
      "Non-trainable params: 2 (8.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_callback = MaskedTextGenerator(sample_tokens, config.TOKEN_DICT['[MASK]'])\n",
    "\n",
    "bert_masked_model = BERT.create_masked_language_bert_model(config)\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 40ms/steplo\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yanni\\Documents\\Documents\\Programmes\\ml_project2\\src\\ml\\samplers\\data_imputation\\BERTs_Playground.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/Documents/Programmes/ml_project2/src/ml/samplers/data_imputation/BERTs_Playground.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m bert_masked_model\u001b[39m.\u001b[39mfit(mlm_ds, epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, callbacks\u001b[39m=\u001b[39m[generator_callback])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/Documents/Programmes/ml_project2/src/ml/samplers/data_imputation/BERTs_Playground.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m bert_masked_model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mbert_mlm.keras\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\yanni\\Documents\\Documents\\Programmes\\ml_project2\\src\\ml\\samplers\\data_imputation\\MaskedTextGenerator.py:35\u001b[0m, in \u001b[0;36mMaskedTextGenerator.on_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     32\u001b[0m prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_tokens)\n\u001b[0;32m     34\u001b[0m masked_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_tokens[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_token_id)\n\u001b[1;32m---> 35\u001b[0m masked_index \u001b[39m=\u001b[39m masked_index[\u001b[39m1\u001b[39m]\n\u001b[0;32m     36\u001b[0m mask_prediction \u001b[39m=\u001b[39m prediction[\u001b[39m0\u001b[39m][masked_index]\n\u001b[0;32m     38\u001b[0m top_indices \u001b[39m=\u001b[39m mask_prediction[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margsort()[\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk :][::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "bert_masked_model.fit(mlm_ds, epochs=5, callbacks=[generator_callback])\n",
    "bert_masked_model.save(\"bert_mlm.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

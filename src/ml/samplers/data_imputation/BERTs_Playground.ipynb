{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT's playground \n",
    "Hello there! Welcome on BERT's playground. You may play with BERT here and see what he can do but always make sure he feels respected and admired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import masking\n",
    "import BERT\n",
    "\n",
    "from Vectorisation import Vectorisation\n",
    "from Config import Config\n",
    "from MaskedLanguageModel import MaskedLanguageModel\n",
    "from MaskedTextGenerator import MaskedTextGenerator\n",
    "\n",
    "with open(\"./ml4science_data.pkl\", \"rb\") as fp:\n",
    "    data_dict = pickle.load(fp)\n",
    "\n",
    "config = Config()\n",
    "vec = Vectorisation(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254, 512) (254, 512) (254, 512)\n",
      "<_BatchDataset element_spec=(TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.int32, name=None), TensorSpec(shape=(None, 512), dtype=tf.float64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for masked language model\n",
    "encoded = vec.encode_dict(data_dict)\n",
    "x_masked_encoded, y_masked_encoded, sample_weights = masking.mask_input_and_labels(encoded, config.TOKEN_DICT)\n",
    "print(x_masked_encoded.shape, y_masked_encoded.shape, sample_weights.shape)\n",
    "\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices((x_masked_encoded, y_masked_encoded, sample_weights))\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)\n",
    "\n",
    "print(mlm_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26  2 26 21 21 26 23 25 21 23  2 21 26 21 21 26 21 25  3  8  3  3  3  3\n",
      "  3  3  3  3  7  8  4  2  8  3 16 20 21 26 21 23 21  3  8  7  8  3  5  8\n",
      "  4  8  2  8  3  8 10  9 10  9  9 15 20 11  9 14 13 11  9 14 13 14 10 11\n",
      "  9  9 13 14 13 14 11  9 10 14 13 10 11  9 14  9 14 15 20 15 15 17 20  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# TODO: continue implementing everything downstream so that BERT can finally run freely on his playground just as he wishes\n",
    "sample_tokens = encoded[0]\n",
    "print(sample_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"masked_bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 512)]                0         []                            \n",
      "                                                                                                  \n",
      " word_embedding (Embedding)  (None, 512, 256)             6912      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 512, 256)             0         ['word_embedding[0][0]']      \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " encoder_0/multiheadattenti  (None, 512, 256)             263168    ['tf.__operators__.add[0][0]',\n",
      " on (MultiHeadAttention)                                             'tf.__operators__.add[0][0]',\n",
      "                                                                     'tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " encoder_0/att_dropout (Dro  (None, 512, 256)             0         ['encoder_0/multiheadattention\n",
      " pout)                                                              [0][0]']                      \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 512, 256)             0         ['tf.__operators__.add[0][0]',\n",
      " OpLambda)                                                           'encoder_0/att_dropout[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_0/att_layernormali  (None, 512, 256)             512       ['tf.__operators__.add_1[0][0]\n",
      " zation (LayerNormalization                                         ']                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " encoder_0/ffn (Sequential)  (None, 512, 256)             65920     ['encoder_0/att_layernormaliza\n",
      "                                                                    tion[0][0]']                  \n",
      "                                                                                                  \n",
      " encoder_0/ffn_dropout (Dro  (None, 512, 256)             0         ['encoder_0/ffn[0][0]']       \n",
      " pout)                                                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 512, 256)             0         ['encoder_0/att_layernormaliza\n",
      " OpLambda)                                                          tion[0][0]',                  \n",
      "                                                                     'encoder_0/ffn_dropout[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_0/ffn_layernormali  (None, 512, 256)             512       ['tf.__operators__.add_2[0][0]\n",
      " zation (LayerNormalization                                         ']                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " mlm_cls (Dense)             (None, 512, 27)              6939      ['encoder_0/ffn_layernormaliza\n",
      "                                                                    tion[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 343965 (1.31 MB)\n",
      "Trainable params: 343963 (1.31 MB)\n",
      "Non-trainable params: 2 (8.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_callback = MaskedTextGenerator(sample_tokens, config.TOKEN_DICT['[MASK]'])\n",
    "\n",
    "bert_masked_model = BERT.create_masked_language_bert_model(config)\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "16/16 [==============================] - ETA: 0s - loss: 3.0659"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer 'tf.__operators__.add' (type TFOpLambda).\n    \n    Dimensions must be equal, but are 32 and 512 for '{{node masked_bert_model/tf.__operators__.add/AddV2}} = AddV2[T=DT_FLOAT](masked_bert_model/word_embedding/embedding_lookup/Identity, masked_bert_model/tf.__operators__.add/AddV2/y)' with input shapes: [32,256], [512,256].\n    \n    Call arguments received by layer 'tf.__operators__.add' (type TFOpLambda):\n      • x=tf.Tensor(shape=(32, 256), dtype=float32)\n      • y=tf.Tensor(shape=(512, 256), dtype=float32)\n      • name=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yanni\\Documents\\Documents\\Programmes\\ml_project2\\src\\ml\\samplers\\data_imputation\\BERTs_Playground.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/Documents/Programmes/ml_project2/src/ml/samplers/data_imputation/BERTs_Playground.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m bert_masked_model\u001b[39m.\u001b[39mfit(mlm_ds, epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, callbacks\u001b[39m=\u001b[39m[generator_callback])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/Documents/Programmes/ml_project2/src/ml/samplers/data_imputation/BERTs_Playground.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m bert_masked_model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mbert_mlm_imdb.h5\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\yanni\\Documents\\Documents\\Programmes\\ml_project2\\src\\ml\\samplers\\data_imputation\\MaskedTextGenerator.py:33\u001b[0m, in \u001b[0;36mMaskedTextGenerator.on_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_epoch_end\u001b[39m(\u001b[39mself\u001b[39m, epoch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     24\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m    Callback method called at the end of each epoch.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m        logs (dict, optional): Dictionary of logs containing the training metrics. Defaults to None.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_tokens)\n\u001b[0;32m     35\u001b[0m     masked_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_tokens \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_token_id)\n\u001b[0;32m     36\u001b[0m     masked_index \u001b[39m=\u001b[39m masked_index[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filefcpx4k84.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer 'tf.__operators__.add' (type TFOpLambda).\n    \n    Dimensions must be equal, but are 32 and 512 for '{{node masked_bert_model/tf.__operators__.add/AddV2}} = AddV2[T=DT_FLOAT](masked_bert_model/word_embedding/embedding_lookup/Identity, masked_bert_model/tf.__operators__.add/AddV2/y)' with input shapes: [32,256], [512,256].\n    \n    Call arguments received by layer 'tf.__operators__.add' (type TFOpLambda):\n      • x=tf.Tensor(shape=(32, 256), dtype=float32)\n      • y=tf.Tensor(shape=(512, 256), dtype=float32)\n      • name=None\n"
     ]
    }
   ],
   "source": [
    "bert_masked_model.fit(mlm_ds, epochs=5, callbacks=[generator_callback])\n",
    "bert_masked_model.save(\"bert_mlm_imdb.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

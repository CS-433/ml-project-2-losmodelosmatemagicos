{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT's playground \n",
    "Hello there! Welcome on BERT's playground. You may play with BERT here and see what he can do but always make sure he feels respected and admired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import masking\n",
    "import BERT\n",
    "\n",
    "from Vectorisation import Vectorisation\n",
    "from Config import Config\n",
    "from MaskedLanguageModel import MaskedLanguageModel\n",
    "from MaskedTextGenerator import MaskedTextGenerator\n",
    "\n",
    "with open(\"./ml4science_data.pkl\", \"rb\") as fp:\n",
    "    data_dict = pickle.load(fp)\n",
    "\n",
    "config = Config()\n",
    "vec = Vectorisation(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254, 128) (254, 128) (254, 128)\n",
      "<_BatchDataset element_spec=(TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), TensorSpec(shape=(None, 128), dtype=tf.float64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for masked language model\n",
    "encoded = vec.encode_dict(data_dict)\n",
    "x_masked_encoded, y_masked_encoded, sample_weights = masking.mask_input_and_labels(encoded, config.TOKEN_DICT)\n",
    "print(x_masked_encoded.shape, y_masked_encoded.shape, sample_weights.shape)\n",
    "\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices((x_masked_encoded, y_masked_encoded, sample_weights))\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)\n",
    "\n",
    "print(mlm_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26  2 26 21 21 26 23 25 21 23  2 21 26 21 21 26 21 25  3  8  3  3  3  3\n",
      "  3  3  3  3  7  8  4  2  8  3 16 20 21 26 21 23 21  3  8  7  8  3  5  8\n",
      "  4  8  2  8  3  8 10  9 10  9  9 15 20 11  9 14 13 11  9 14 13 14 10 11\n",
      "  9  9 13 14 13 14 11  9 10 14 13 10 11  9 14  9 14 15 20 15 15 17 20  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# TODO: continue implementing everything downstream so that BERT can finally run freely on his playground just as he wishes\n",
    "sample_tokens = x_masked_encoded\n",
    "print(encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26  2 26 21 21 26  9 25 21 23  2 21 26 21 21 26 21  1  3  8  3  1  3  3\n",
      "  3  3  3  3  7  8  1  2  8  3 16 20 21 26 21 23 21  3  8  7  8  3  5  8\n",
      "  4  1  2  8  3  1 10  9 10  9  9 15  1 11  9  1  1 11  9 14 13 14 10  1\n",
      "  9  9 13 14 13 14 11  9 10 14 13 10 11  1 14  9  1 15 20 15 15 17 20  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(x_masked_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"masked_bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " word_embedding (Embedding)  (None, 128, 256)             6912      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 128, 256)             0         ['word_embedding[0][0]']      \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " encoder_0/multiheadattenti  (None, 128, 256)             263168    ['tf.__operators__.add[0][0]',\n",
      " on (MultiHeadAttention)                                             'tf.__operators__.add[0][0]',\n",
      "                                                                     'tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " encoder_0/att_dropout (Dro  (None, 128, 256)             0         ['encoder_0/multiheadattention\n",
      " pout)                                                              [0][0]']                      \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 128, 256)             0         ['tf.__operators__.add[0][0]',\n",
      " OpLambda)                                                           'encoder_0/att_dropout[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_0/att_layernormali  (None, 128, 256)             512       ['tf.__operators__.add_1[0][0]\n",
      " zation (LayerNormalization                                         ']                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " encoder_0/ffn (Sequential)  (None, 128, 256)             33088     ['encoder_0/att_layernormaliza\n",
      "                                                                    tion[0][0]']                  \n",
      "                                                                                                  \n",
      " encoder_0/ffn_dropout (Dro  (None, 128, 256)             0         ['encoder_0/ffn[0][0]']       \n",
      " pout)                                                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 128, 256)             0         ['encoder_0/att_layernormaliza\n",
      " OpLambda)                                                          tion[0][0]',                  \n",
      "                                                                     'encoder_0/ffn_dropout[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_0/ffn_layernormali  (None, 128, 256)             512       ['tf.__operators__.add_2[0][0]\n",
      " zation (LayerNormalization                                         ']                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " mlm_cls (Dense)             (None, 128, 27)              6939      ['encoder_0/ffn_layernormaliza\n",
      "                                                                    tion[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 311133 (1.19 MB)\n",
      "Trainable params: 311131 (1.19 MB)\n",
      "Non-trainable params: 2 (8.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_callback = MaskedTextGenerator(sample_tokens, config.TOKEN_DICT['[MASK]'])\n",
    "\n",
    "bert_masked_model = BERT.create_masked_language_bert_model(config)\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "8/8 [==============================] - 0s 34ms/steploss\n",
      "prediction shape: (254, 128, 27)\n",
      "mask pred shape: (11, 27)\n",
      "best results shape: (5, 3)\n",
      "\n",
      "masked nb: \t 1                2                3                4                5                \n",
      "predictions: \t[26 22 14]       [26 22  9]       [22 21 14]       [14 22  9]       [22 14 10]       \n",
      "probabilities: \t[0.23 0.18 0.11] [0.24 0.16 0.12] [0.19 0.17 0.11] [0.19 0.18 0.11] [0.23 0.1  0.09] \n",
      "\n",
      "16/16 [==============================] - 3s 72ms/step - loss: 2.9095\n",
      "Epoch 2/5\n",
      "8/8 [==============================] - 0s 30ms/steploss\n",
      "prediction shape: (254, 128, 27)\n",
      "mask pred shape: (11, 27)\n",
      "best results shape: (5, 3)\n",
      "\n",
      "masked nb: \t 1                2                3                4                5                \n",
      "predictions: \t[26 21 23]       [26  9 21]       [21 14 10]       [14  9 10]       [10 14  9]       \n",
      "probabilities: \t[0.26 0.15 0.14] [0.24 0.19 0.16] [0.17 0.13 0.11] [0.19 0.15 0.13] [0.13 0.11 0.11] \n",
      "\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 2.6210\n",
      "Epoch 3/5\n",
      "8/8 [==============================] - 0s 24ms/steploss: \n",
      "prediction shape: (254, 128, 27)\n",
      "mask pred shape: (11, 27)\n",
      "best results shape: (5, 3)\n",
      "\n",
      "masked nb: \t 1                2                3                4                5                \n",
      "predictions: \t[21 26 23]       [21 26 23]       [21 15 22]       [14 15 21]       [20 21 10]       \n",
      "probabilities: \t[0.24 0.22 0.16] [0.26 0.23 0.09] [0.22 0.14 0.09] [0.12 0.11 0.1 ] [0.13 0.11 0.1 ] \n",
      "\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 2.5492\n",
      "Epoch 4/5\n",
      "8/8 [==============================] - 0s 26ms/steploss\n",
      "prediction shape: (254, 128, 27)\n",
      "mask pred shape: (11, 27)\n",
      "best results shape: (5, 3)\n",
      "\n",
      "masked nb: \t 1                2                3                4                5                \n",
      "predictions: \t[26 22 23]       [26 22  9]       [22 21 26]       [22 14  9]       [22 20 10]       \n",
      "probabilities: \t[0.29 0.22 0.11] [0.29 0.18 0.13] [0.33 0.1  0.08] [0.26 0.14 0.1 ] [0.28 0.09 0.08] \n",
      "\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 2.5101\n",
      "Epoch 5/5\n",
      "8/8 [==============================] - 0s 24ms/steploss: \n",
      "prediction shape: (254, 128, 27)\n",
      "mask pred shape: (11, 27)\n",
      "best results shape: (5, 3)\n",
      "\n",
      "masked nb: \t 1                2                3                4                5                \n",
      "predictions: \t[26 21 23]       [26 21  9]       [22 21 14]       [14  9 22]       [14 22  9]       \n",
      "probabilities: \t[0.28 0.22 0.12] [0.26 0.23 0.15] [0.17 0.16 0.14] [0.21 0.17 0.13] [0.13 0.12 0.1 ] \n",
      "\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 2.4286\n"
     ]
    }
   ],
   "source": [
    "bert_masked_model.fit(mlm_ds, epochs=5, callbacks=[generator_callback])\n",
    "bert_masked_model.save(\"bert_models/bert_mlm.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

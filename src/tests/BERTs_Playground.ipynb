{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT's playground \n",
    "Hello there! Welcome on BERT's playground. You may play with BERT here and see what he can do but always make sure he feels respected and admired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../ml/BERT')\n",
    "import masking\n",
    "import BERT\n",
    "from Vectorisation import Vectorisation\n",
    "from Config import Config\n",
    "from MaskedTextGenerator import MaskedTextGenerator\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "with open(\"../../data/ml4science_data.pkl\", \"rb\") as fp:\n",
    "    data_dict = pickle.load(fp)\n",
    "\n",
    "config = Config()\n",
    "vec = Vectorisation(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254, 128) (254, 128) (254, 128)\n",
      "<_BatchDataset element_spec=(TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), TensorSpec(shape=(None, 128), dtype=tf.float64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for masked language model\n",
    "encoded = vec.encode_dict(data_dict)\n",
    "x_masked_encoded, y_masked_encoded, sample_weights = masking.mask_input_and_labels(encoded, config.TOKEN_DICT, seed=32)\n",
    "print(x_masked_encoded.shape, y_masked_encoded.shape, sample_weights.shape)\n",
    "\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices((x_masked_encoded, y_masked_encoded, sample_weights))\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)\n",
    "\n",
    "print(mlm_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26,  2, 26, ...,  0,  0,  0],\n",
       "       [26, 21, 26, ..., 14, 11, 14],\n",
       "       [25, 26, 21, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [26,  2, 26, ...,  0,  0,  0],\n",
       "       [26, 25, 26, ...,  0,  0,  0],\n",
       "       [26,  2, 26, ..., 22, 26, 21]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26  2 26 21 21 26 23 25 21 23  2 21 26 21 21 26 21 25  3  8  3  3  3  3\n",
      "  3  3  3  3  7  8  4  2  8  3 16 20 21 26 21 23 21  3  8  7  8  3  5  8\n",
      "  4  8  2  8  3  8 10  9 10  9  9 15 20 11  9 14 13 11  9 14 13 14 10 11\n",
      "  9  9 13 14 13 14 11  9 10 14 13 10 11  9 14  9 14 15 20 15 15 17 20  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "sample_tokens = x_masked_encoded[0:1]\n",
    "print(y_masked_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26  2 26 21 21 26  9 25 21 23  2 21 26 21 21 26 21  1  3  8  3  1  3  3\n",
      "  3  3  3  3  7  8  1  2  8  3 16 20 21 26 21 23 21  3  8  7  8  3  5  8\n",
      "  4  1  2  8  3  1 10  9 10  9  9 15  1 11  9  1  1 11  9 14 13 14 10  1\n",
      "  9  9 13 14 13 14 11  9 10 14 13 10 11  1 14  9  1 15 20 15 15 17 20  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(x_masked_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"masked_bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " word_embedding (Embedding)  (None, 128, 64)              1728      ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TF  (None, 128, 64)              0         ['word_embedding[0][0]']      \n",
      " OpLambda)                                                                                        \n",
      "                                                                                                  \n",
      " encoder_0/multiheadattenti  (None, 128, 64)              16640     ['tf.__operators__.add_6[0][0]\n",
      " on (MultiHeadAttention)                                            ',                            \n",
      "                                                                     'tf.__operators__.add_6[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'tf.__operators__.add_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " encoder_0/att_dropout (Dro  (None, 128, 64)              0         ['encoder_0/multiheadattention\n",
      " pout)                                                              [0][0]']                      \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TF  (None, 128, 64)              0         ['tf.__operators__.add_6[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'encoder_0/att_dropout[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_0/att_layernormali  (None, 128, 64)              128       ['tf.__operators__.add_7[0][0]\n",
      " zation (LayerNormalization                                         ']                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " encoder_0/ffn (Sequential)  (None, 128, 64)              8320      ['encoder_0/att_layernormaliza\n",
      "                                                                    tion[0][0]']                  \n",
      "                                                                                                  \n",
      " encoder_0/ffn_dropout (Dro  (None, 128, 64)              0         ['encoder_0/ffn[0][0]']       \n",
      " pout)                                                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TF  (None, 128, 64)              0         ['encoder_0/att_layernormaliza\n",
      " OpLambda)                                                          tion[0][0]',                  \n",
      "                                                                     'encoder_0/ffn_dropout[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_0/ffn_layernormali  (None, 128, 64)              128       ['tf.__operators__.add_8[0][0]\n",
      " zation (LayerNormalization                                         ']                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " mlm_cls (Dense)             (None, 128, 27)              1755      ['encoder_0/ffn_layernormaliza\n",
      "                                                                    tion[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 28701 (112.11 KB)\n",
      "Trainable params: 28699 (112.11 KB)\n",
      "Non-trainable params: 2 (8.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_callback = MaskedTextGenerator(sample_tokens, config.TOKEN_DICT['[MASK]'])\n",
    "\n",
    "bert_masked_model = BERT.create_masked_language_bert_model(config)\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 2s 17ms/step - loss: 3.0689\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 2.6844\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 2.6046\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.5594\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.5306\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.5012\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.4747\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.4391\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.3903\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.2766\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.1689\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.0852\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 2.0321\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.9741\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.9251\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.8732\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 1.8339\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 1.7747\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 1.7221\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.6884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1769a78c610>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A callback in Keras is a function that is called at certain points during the training process. -> Here is called after each epoch during the training\n",
    "# Here we use to see the \"performance\" at each epoch while predicting on a \"test set\" aka the sample_tokens\n",
    "\n",
    "bert_masked_model.fit(mlm_ds, epochs=20)#, callbacks=[generator_callback]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 131ms/step\n",
      "Predictions:\n",
      " [[26 26 26 21 21 26 21 21 21 23 21 21 26 21 21 26 21 23 23 21 21  9  9  9\n",
      "   9  9  9  9 15 15 15 15 15  9 15 20 15 15 15  9  9  9  8 15  8 15  8  8\n",
      "   9  9  9  9  9  9 10  9 10  9  9  9 20 11  9  9  9  9  9 14 10 14 10 14\n",
      "   9  9 10 14 10 14 14  9 10 14  9  9  9  9 14  9 14 14 20 16 16 17 20 20\n",
      "  20 20  9  9  9 20 20 20 15 20 20 20 20 20 20 15 15 15 15 15 15 11 11 11\n",
      "   9  9  9  9  9  9  9  9]]\n",
      "Original:\n",
      " [[26  2 26 21 21 26 23 25 21 23  2 21 26 21 21 26 21 25  3  8  3  3  3  3\n",
      "   3  3  3  3  7  8  4  2  8  3 16 20 21 26 21 23 21  3  8  7  8  3  5  8\n",
      "   4  8  2  8  3  8 10  9 10  9  9 15 20 11  9 14 13 11  9 14 13 14 10 11\n",
      "   9  9 13 14 13 14 11  9 10 14 13 10 11  9 14  9 14 15 20 15 15 17 20  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]]\n",
      "Masked:\n",
      " [[26  2 26 21 21 26  9 25 21 23  2 21 26 21 21 26 21  1  3  8  3  1  3  3\n",
      "   3  3  3  3  7  8  1  2  8  3 16 20 21 26 21 23 21  3  8  7  8  3  5  8\n",
      "   4  1  2  8  3  1 10  9 10  9  9 15  1 11  9  1  1 11  9 14 13 14 10  1\n",
      "   9  9 13 14 13 14 11  9 10 14 13 10 11  1 14  9  1 15 20 15 15 17 20  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "predictions = bert_masked_model.predict(x_masked_encoded[0:1])\n",
    "\n",
    "predictions_max = np.argmax(predictions, axis=2)\n",
    "print(\"Predictions:\\n\",predictions_max)\n",
    "print(\"Original:\\n\", y_masked_encoded[0:1])\n",
    "print(\"Masked:\\n\", x_masked_encoded[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True False  True  True  True  True False False  True  True False  True\n",
      "   True  True  True  True  True False False False False False False False\n",
      "  False False False False False False False False False False False  True\n",
      "  False False False False False False  True False  True False False  True\n",
      "  False False False False False False  True  True  True  True  True False\n",
      "   True  True  True False False False  True  True False  True  True False\n",
      "   True  True False  True False  True False  True  True  True False False\n",
      "  False  True  True  True  True False  True False False  True  True False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False]]\n",
      "Accuracy brut:  0.43\n",
      "Accuracy without taking in acount padding:  0.45263157894736844\n",
      "Accuracy on masked tokens:  0.2727272727272727\n"
     ]
    }
   ],
   "source": [
    "print(predictions_max == y_masked_encoded[0:1])\n",
    "# print accuracy\n",
    "print(\"Accuracy brut: \", np.sum(predictions_max == y_masked_encoded[0:1]) / (100*len(y_masked_encoded[0:1])))\n",
    "# print accuracy without padding\n",
    "print(\"Accuracy without taking in acount padding: \", np.sum((predictions_max == y_masked_encoded[0:1]) * (y_masked_encoded[0:1] != 0)) / np.sum(y_masked_encoded[0:1] != 0))\n",
    "print(\"Accuracy on masked tokens: \", np.sum((predictions_max == y_masked_encoded[0:1]) * (x_masked_encoded[0:1] == config.TOKEN_DICT['[MASK]'])) / np.sum(x_masked_encoded[0:1] == config.TOKEN_DICT['[MASK]']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def get_end_to_end(model):\\n    inputs = keras.Input(shape=(None,))\\n    outputs = model(inputs)\\n    reshaped_outputs = keras.layers.Lambda(lambda x: keras.backend.argmax(x, axis=-1))(outputs)\\n    end_to_end_model = keras.Model(inputs, reshaped_outputs, name=\"end_to_end_model\")\\n    optimizer = keras.optimizers.Adam(learning_rate=config.bert.LR)\\n    end_to_end_model.compile(\\n        optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\\n    )\\n    return end_to_end_model\\n\\nend_to_end_classification_model = get_end_to_end(bert_masked_model)\\n\\n# Build dataset for end to end model input (will be used at the end)\\ntest_raw_classifier_ds = tf.data.Dataset.from_tensor_slices((x_masked_encoded, y_masked_encoded)).batch(config.BATCH_SIZE)\\n\\nend_to_end_classification_model.evaluate(test_raw_classifier_ds)'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This how we can load a Keras model\n",
    "\"\"\"# Load OUR pretrained bert model\n",
    "mlm_model = keras.models.load_model(\n",
    "    \"bert_mlm_imdb.keras\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel}\n",
    ")\"\"\"\n",
    "\n",
    "# Kinda failed attempt to create a end to end model (we don't really need it) -> But it's a good example of how to create a model with a custom loss function and reshape the output\n",
    "\"\"\"def get_end_to_end(model):\n",
    "    inputs = keras.Input(shape=(None,))\n",
    "    outputs = model(inputs)\n",
    "    reshaped_outputs = keras.layers.Lambda(lambda x: keras.backend.argmax(x, axis=-1))(outputs)\n",
    "    end_to_end_model = keras.Model(inputs, reshaped_outputs, name=\"end_to_end_model\")\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.bert.LR)\n",
    "    end_to_end_model.compile(\n",
    "        optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return end_to_end_model\n",
    "\n",
    "end_to_end_classification_model = get_end_to_end(bert_masked_model)\n",
    "\n",
    "# Build dataset for end to end model input (will be used at the end)\n",
    "test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices((x_masked_encoded, y_masked_encoded)).batch(config.BATCH_SIZE)\n",
    "\n",
    "end_to_end_classification_model.evaluate(test_raw_classifier_ds)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT's playground \n",
    "Hello there! Welcome on BERT's playground. You may play with BERT here and see what he can do but always make sure he feels respected and admired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../ml/BERT')\n",
    "import masking\n",
    "import BERT\n",
    "from Vectorisation import Vectorisation\n",
    "from Config import Config\n",
    "from MaskedTextGenerator import MaskedTextGenerator\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "with open(\"../../data/ml4science_data.pkl\", \"rb\") as fp:\n",
    "    data_dict = pickle.load(fp)\n",
    "\n",
    "config = Config()\n",
    "vec = Vectorisation(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254, 128) (254, 128) (254, 128)\n",
      "<_BatchDataset element_spec=(TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), TensorSpec(shape=(None, 128), dtype=tf.float64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for masked language model\n",
    "encoded = vec.encode_dict(data_dict)\n",
    "x_masked_encoded, y_masked_encoded, sample_weights = masking.mask_input_and_labels(encoded, config.TOKEN_DICT, seed=32)\n",
    "print(x_masked_encoded.shape, y_masked_encoded.shape, sample_weights.shape)\n",
    "\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices((x_masked_encoded, y_masked_encoded, sample_weights))\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)\n",
    "\n",
    "print(mlm_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26,  2, 26, ...,  0,  0,  0],\n",
       "       [26, 21, 26, ..., 14, 11, 14],\n",
       "       [25, 26, 21, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [26,  2, 26, ...,  0,  0,  0],\n",
       "       [26, 25, 26, ...,  0,  0,  0],\n",
       "       [26,  2, 26, ..., 22, 26, 21]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26  2 26 21 21 26 23 25 21 23  2 21 26 21 21 26 21 25  3  8  3  3  3  3\n",
      "  3  3  3  3  7  8  4  2  8  3 16 20 21 26 21 23 21  3  8  7  8  3  5  8\n",
      "  4  8  2  8  3  8 10  9 10  9  9 15 20 11  9 14 13 11  9 14 13 14 10 11\n",
      "  9  9 13 14 13 14 11  9 10 14 13 10 11  9 14  9 14 15 20 15 15 17 20  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "sample_tokens = x_masked_encoded[0:1]\n",
    "print(y_masked_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26  2 26 21 21 26  9 25 21 23  2 21 26 21 21 26 21  1  3  8  3  1  3  3\n",
      "  3  3  3  3  7  8  1  2  8  3 16 20 21 26 21 23 21  3  8  7  8  3  5  8\n",
      "  4  1  2  8  3  1 10  9 10  9  9 15  1 11  9  1  1 11  9 14 13 14 10  1\n",
      "  9  9 13 14 13 14 11  9 10 14 13 10 11  1 14  9  1 15 20 15 15 17 20  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(x_masked_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"masked_bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " word_embedding (Embedding)  (None, 128, 64)              1728      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 128, 64)              0         ['word_embedding[0][0]']      \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " encoder_0/multiheadattenti  (None, 128, 64)              16640     ['tf.__operators__.add[0][0]',\n",
      " on (MultiHeadAttention)                                             'tf.__operators__.add[0][0]',\n",
      "                                                                     'tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " encoder_0/att_dropout (Dro  (None, 128, 64)              0         ['encoder_0/multiheadattention\n",
      " pout)                                                              [0][0]']                      \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 128, 64)              0         ['tf.__operators__.add[0][0]',\n",
      " OpLambda)                                                           'encoder_0/att_dropout[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_0/att_layernormali  (None, 128, 64)              128       ['tf.__operators__.add_1[0][0]\n",
      " zation (LayerNormalization                                         ']                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " encoder_0/ffn (Sequential)  (None, 128, 64)              8320      ['encoder_0/att_layernormaliza\n",
      "                                                                    tion[0][0]']                  \n",
      "                                                                                                  \n",
      " encoder_0/ffn_dropout (Dro  (None, 128, 64)              0         ['encoder_0/ffn[0][0]']       \n",
      " pout)                                                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 128, 64)              0         ['encoder_0/att_layernormaliza\n",
      " OpLambda)                                                          tion[0][0]',                  \n",
      "                                                                     'encoder_0/ffn_dropout[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_0/ffn_layernormali  (None, 128, 64)              128       ['tf.__operators__.add_2[0][0]\n",
      " zation (LayerNormalization                                         ']                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " mlm_cls (Dense)             (None, 128, 27)              1755      ['encoder_0/ffn_layernormaliza\n",
      "                                                                    tion[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 28701 (112.11 KB)\n",
      "Trainable params: 28699 (112.11 KB)\n",
      "Non-trainable params: 2 (8.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_callback = MaskedTextGenerator(sample_tokens, config.TOKEN_DICT['[MASK]'])\n",
    "\n",
    "bert_masked_model = BERT.create_masked_language_bert_model(config)\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 2s 18ms/step - loss: 3.2364\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 2.7211\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.6188\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.5628\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 2.5261\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.5169\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.4786\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.4564\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.4502\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 2.3838\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 2.3181\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 2.2010\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 2.0641\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.9840\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.9212\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.8691\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.7990\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.7671\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.7259\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.6930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1769a1f0bd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A callback in Keras is a function that is called at certain points during the training process. -> Here is called after each epoch during the training\n",
    "# Here we use to see the \"performance\" at each epoch while predicting on a \"test set\" aka the sample_tokens\n",
    "\n",
    "bert_masked_model.fit(mlm_ds, epochs=20)#, callbacks=[generator_callback]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 175ms/step\n",
      "Predictions:\n",
      " [[26 26 26 21 21 26 21 21 21 21 21 21 26 21 21 26 21 26 26 21 21  9  9  9\n",
      "   9  9  9  9  9  9  9 14  9  9 16 20 21 26 21  9 21  9 11  9  8 15 14  8\n",
      "   9  9 14  8  9  9 10  9 10  9 14 14 14 14 10 14 14 14  9 14 10 14 10  9\n",
      "   9  9  9 14 10 14 14  9 10 14  9  9  9  9 14  9 14 15 20 16 16 20 20 20\n",
      "  20 20 14 14 14 14 14 20 20 20 20 20 20 20  4  4 15 15 15 15 15  4  4  4\n",
      "   4  9 14  4  4 14  9  9]]\n",
      "Original:\n",
      " [[26  2 26 21 21 26 23 25 21 23  2 21 26 21 21 26 21 25  3  8  3  3  3  3\n",
      "   3  3  3  3  7  8  4  2  8  3 16 20 21 26 21 23 21  3  8  7  8  3  5  8\n",
      "   4  8  2  8  3  8 10  9 10  9  9 15 20 11  9 14 13 11  9 14 13 14 10 11\n",
      "   9  9 13 14 13 14 11  9 10 14 13 10 11  9 14  9 14 15 20 15 15 17 20  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]]\n",
      "Masked:\n",
      " [[26  2 26 21 21 26  9 25 21 23  2 21 26 21 21 26 21  1  3  8  3  1  3  3\n",
      "   3  3  3  3  7  8  1  2  8  3 16 20 21 26 21 23 21  3  8  7  8  3  5  8\n",
      "   4  1  2  8  3  1 10  9 10  9  9 15  1 11  9  1  1 11  9 14 13 14 10  1\n",
      "   9  9 13 14 13 14 11  9 10 14 13 10 11  1 14  9  1 15 20 15 15 17 20  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "predictions = bert_masked_model.predict(x_masked_encoded[0:1])\n",
    "\n",
    "predictions_max = np.argmax(predictions, axis=2)\n",
    "print(\"Predictions:\\n\",predictions_max)\n",
    "print(\"Original:\\n\", y_masked_encoded[0:1])\n",
    "print(\"Masked:\\n\", x_masked_encoded[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True False  True  True  True  True False False  True False False  True\n",
      "   True  True  True  True  True False False False False False False False\n",
      "  False False False False False False False False False False  True  True\n",
      "   True  True  True False  True False False False  True False False  True\n",
      "  False False False  True False False  True  True  True  True False False\n",
      "  False False False  True False False  True  True False  True  True False\n",
      "   True  True False  True False  True False  True  True  True False False\n",
      "  False  True  True  True  True  True  True False False False  True False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False]]\n",
      "Accuracy brut:  0.44\n",
      "Accuracy without taking in acount padding:  0.4631578947368421\n",
      "Accuracy on masked tokens:  0.2727272727272727\n"
     ]
    }
   ],
   "source": [
    "print(predictions_max == y_masked_encoded[0:1])\n",
    "# print accuracy\n",
    "print(\"Accuracy brut: \", np.sum(predictions_max == y_masked_encoded[0:1]) / (100*len(y_masked_encoded[0:1])))\n",
    "# print accuracy without padding\n",
    "print(\"Accuracy without taking in acount padding: \", np.sum((predictions_max == y_masked_encoded[0:1]) * (y_masked_encoded[0:1] != 0)) / np.sum(y_masked_encoded[0:1] != 0))\n",
    "print(\"Accuracy on masked tokens: \", np.sum((predictions_max == y_masked_encoded[0:1]) * (x_masked_encoded[0:1] == config.TOKEN_DICT['[MASK]'])) / np.sum(x_masked_encoded[0:1] == config.TOKEN_DICT['[MASK]']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def get_end_to_end(model):\\n    inputs = keras.Input(shape=(None,))\\n    outputs = model(inputs)\\n    reshaped_outputs = keras.layers.Lambda(lambda x: keras.backend.argmax(x, axis=-1))(outputs)\\n    end_to_end_model = keras.Model(inputs, reshaped_outputs, name=\"end_to_end_model\")\\n    optimizer = keras.optimizers.Adam(learning_rate=config.bert.LR)\\n    end_to_end_model.compile(\\n        optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\\n    )\\n    return end_to_end_model\\n\\nend_to_end_classification_model = get_end_to_end(bert_masked_model)\\n\\n# Build dataset for end to end model input (will be used at the end)\\ntest_raw_classifier_ds = tf.data.Dataset.from_tensor_slices((x_masked_encoded, y_masked_encoded)).batch(config.BATCH_SIZE)\\n\\nend_to_end_classification_model.evaluate(test_raw_classifier_ds)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This how we can load a Keras model\n",
    "\"\"\"# Load OUR pretrained bert model\n",
    "mlm_model = keras.models.load_model(\n",
    "    \"bert_mlm_imdb.keras\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel}\n",
    ")\"\"\"\n",
    "\n",
    "# Kinda failed attempt to create a end to end model (we don't really need it) -> But it's a good example of how to create a model with a custom loss function and reshape the output\n",
    "\"\"\"def get_end_to_end(model):\n",
    "    inputs = keras.Input(shape=(None,))\n",
    "    outputs = model(inputs)\n",
    "    reshaped_outputs = keras.layers.Lambda(lambda x: keras.backend.argmax(x, axis=-1))(outputs)\n",
    "    end_to_end_model = keras.Model(inputs, reshaped_outputs, name=\"end_to_end_model\")\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.bert.LR)\n",
    "    end_to_end_model.compile(\n",
    "        optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return end_to_end_model\n",
    "\n",
    "end_to_end_classification_model = get_end_to_end(bert_masked_model)\n",
    "\n",
    "# Build dataset for end to end model input (will be used at the end)\n",
    "test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices((x_masked_encoded, y_masked_encoded)).batch(config.BATCH_SIZE)\n",
    "\n",
    "end_to_end_classification_model.evaluate(test_raw_classifier_ds)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing BertPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 1, 0, 0, 0, 0, 0, 7.889],\n",
       " [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 75.141, 0.0],\n",
       " [0.0, 0.0, 0.0, 1.0, 0, 0, 0, 0, 0, 3.25]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the sequences used in src\\script_oversample.py\n",
    "with open('../../data/sequences.pkl', 'rb') as f:\n",
    "    sequences = pickle.load(f)\n",
    "\n",
    "sequences[0][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254, 128) [26 25 26 21 21 26 23 25 21 23 25 21 26 21 21 26 21 25  3  8  3  3  3  3\n",
      "  3  3  3  3  7  8  4  7  8  3 16 20 21 26 21 23 21  3  8  7  8  3  5  8\n",
      "  4  8  7  8  3  8 10  9 10  9  9 15 20 11  9 14 13 11  9 14 13 14 10 11\n",
      "  9  9 13 14 13 14 11  9 10 14 13 10 11  9 14  9 14 15 20 15 15 17 20  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# Test with not SEP because unsing only sequences not info of dictionarry\n",
    "sequences = vec.encode(sequences)\n",
    "print(sequences.shape,sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254, 128) (254, 128) (254, 128)\n",
      "<_BatchDataset element_spec=(TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), TensorSpec(shape=(None, 128), dtype=tf.float64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "x_masked_encoded, y_masked_encoded, sample_weights = masking.mask_input_and_labels(sequences, config.TOKEN_DICT)\n",
    "print(x_masked_encoded.shape, y_masked_encoded.shape, sample_weights.shape)\n",
    "\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices((x_masked_encoded, y_masked_encoded, sample_weights))\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)\n",
    "print(mlm_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`y` argument is not supported when using dataset as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m bert_masked_model \u001b[38;5;241m=\u001b[39m BERT\u001b[38;5;241m.\u001b[39mcreate_masked_language_bert_model(config)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#bert_masked_model.summary()\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m bert_masked_model\u001b[38;5;241m.\u001b[39mfit(mlm_ds, config\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39mepoch)\n",
      "File \u001b[1;32mc:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\yanni\\.conda\\envs\\ML\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:808\u001b[0m, in \u001b[0;36mDatasetAdapter._validate_args\u001b[1;34m(self, y, sample_weights, steps, pss_evaluation_shards)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Arguments that shouldn't be passed.\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_none_or_empty(y):\n\u001b[1;32m--> 808\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    809\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`y` argument is not supported when using dataset as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    810\u001b[0m     )\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_none_or_empty(sample_weights):\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`sample_weight` argument is not supported when using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    815\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: `y` argument is not supported when using dataset as input."
     ]
    }
   ],
   "source": [
    "bert_masked_model = BERT.create_masked_language_bert_model(config)\n",
    "#bert_masked_model.summary()\n",
    "\n",
    "bert_masked_model.fit(mlm_ds, config.bert.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"masked_bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " word_embedding (Embedding)  (None, 128, 64)              1728      ['input_4[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TF  (None, 128, 64)              0         ['word_embedding[0][0]']      \n",
      " OpLambda)                                                                                        \n",
      "                                                                                                  \n",
      " encoder_0/multiheadattenti  (None, 128, 64)              16640     ['tf.__operators__.add_9[0][0]\n",
      " on (MultiHeadAttention)                                            ',                            \n",
      "                                                                     'tf.__operators__.add_9[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'tf.__operators__.add_9[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " encoder_0/att_dropout (Dro  (None, 128, 64)              0         ['encoder_0/multiheadattention\n",
      " pout)                                                              [0][0]']                      \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (T  (None, 128, 64)              0         ['tf.__operators__.add_9[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'encoder_0/att_dropout[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_0/att_layernormali  (None, 128, 64)              128       ['tf.__operators__.add_10[0][0\n",
      " zation (LayerNormalization                                         ]']                           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " encoder_0/ffn (Sequential)  (None, 128, 64)              8320      ['encoder_0/att_layernormaliza\n",
      "                                                                    tion[0][0]']                  \n",
      "                                                                                                  \n",
      " encoder_0/ffn_dropout (Dro  (None, 128, 64)              0         ['encoder_0/ffn[0][0]']       \n",
      " pout)                                                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (T  (None, 128, 64)              0         ['encoder_0/att_layernormaliza\n",
      " FOpLambda)                                                         tion[0][0]',                  \n",
      "                                                                     'encoder_0/ffn_dropout[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_0/ffn_layernormali  (None, 128, 64)              128       ['tf.__operators__.add_11[0][0\n",
      " zation (LayerNormalization                                         ]']                           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " mlm_cls (Dense)             (None, 128, 27)              1755      ['encoder_0/ffn_layernormaliza\n",
      "                                                                    tion[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 28701 (112.11 KB)\n",
      "Trainable params: 28699 (112.11 KB)\n",
      "Non-trainable params: 2 (8.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`y` argument is not supported when using dataset as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBertPipline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertPipline\n\u001b[0;32m      3\u001b[0m bert \u001b[38;5;241m=\u001b[39m BertPipline(config, vec)\n\u001b[1;32m----> 4\u001b[0m bert_masked_model \u001b[38;5;241m=\u001b[39m \u001b[43mbert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\david\\Documents\\GitHub\\ml2\\src\\ml\\samplers\\data_imputation\\BertPipline.py:33\u001b[0m, in \u001b[0;36mBertPipline.train\u001b[1;34m(self, sequences)\u001b[0m\n\u001b[0;32m     30\u001b[0m bert_masked_model \u001b[38;5;241m=\u001b[39m BERT\u001b[38;5;241m.\u001b[39mcreate_masked_language_bert_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[0;32m     31\u001b[0m bert_masked_model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m---> 33\u001b[0m \u001b[43mbert_masked_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlm_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# No need of callbacks\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# No know yet if we want to save the model\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# bert_masked_model.save(\"bert_models/bert_mlm.keras\") \u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bert_masked_model\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\Jade\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\Jade\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:808\u001b[0m, in \u001b[0;36mDatasetAdapter._validate_args\u001b[1;34m(self, y, sample_weights, steps, pss_evaluation_shards)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Arguments that shouldn't be passed.\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_none_or_empty(y):\n\u001b[1;32m--> 808\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    809\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`y` argument is not supported when using dataset as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    810\u001b[0m     )\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_none_or_empty(sample_weights):\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`sample_weight` argument is not supported when using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    815\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: `y` argument is not supported when using dataset as input."
     ]
    }
   ],
   "source": [
    "from BertPipeline import BertPipeline\n",
    "\n",
    "bert = BertPipeline(config, vec)\n",
    "bert_masked_model = bert.train(sequences)\n",
    "\n",
    "new_sequences = bert.predict(encoded, bert_masked_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
